# ğŸ“˜ AmbedkarGPT â€“ RAG Evaluation Framework (Assignment 2)

This repository contains **Assignment 2** of the AmbedkarGPT Intern Task.

The goal is to build a **complete evaluation system** for a Retrieval-Augmented Generation (RAG) pipeline using:

- **LangChain**
- **ChromaDB** (local vector database)
- **HuggingFace Embeddings** (sentence-transformers/all-MiniLM-L6-v2)
- **Ollama + Mistral 7B** (offline LLM)
- **Evaluation Metrics** (ROUGE, BLEU, Cosine Similarity, Hit Rate, MRR, etc.)

The system automatically evaluates the RAG pipeline using **25 predefined questions** and compares **three chunking strategies**.

---

# ğŸ§  What This Project Does

This project automatically:

1. Loads the **6 Ambedkar speech documents**.
2. Splits them into **three chunk sizes**:
   - Small (200â€“300 characters)
   - Medium (500â€“600 characters)
   - Large (800â€“1000 characters)
3. Builds **three Chroma vector databases**.
4. Runs RAG for **25 test questions**.
5. Generates answers using **Mistral 7B (via Ollama)**.
6. Computes:
   - Hit Rate
   - Mean Reciprocal Rank (MRR)
   - Precision@K
   - ROUGE-L
   - BLEU
   - Cosine Similarity
   - Faithfulness
   - Relevance
7. Saves results under `/results/`
8. Generates a combined report
9. (You write the final analysis in results_analysis.md)

---

# ğŸ“ Project Structure

AmbedkarGPT-Intern-Task/
â”‚
â”œâ”€â”€ corpus/
â”‚ â”œâ”€â”€ speech1.txt
â”‚ â”œâ”€â”€ speech2.txt
â”‚ â”œâ”€â”€ speech3.txt
â”‚ â”œâ”€â”€ speech4.txt
â”‚ â”œâ”€â”€ speech5.txt
â”‚ â””â”€â”€ speech6.txt
â”‚
â”œâ”€â”€ test_dataset.json
â”‚
â”œâ”€â”€ evaluation.py
â”œâ”€â”€ utils.py
â”œâ”€â”€ metrics.py
â”‚
â”œâ”€â”€ db_small/
â”œâ”€â”€ db_medium/
â”œâ”€â”€ db_large/
â”‚
â”œâ”€â”€ results/
â”‚ â”œâ”€â”€ results_small.json
â”‚ â”œâ”€â”€ results_medium.json
â”‚ â”œâ”€â”€ results_large.json
â”‚ â””â”€â”€ combined_report.json
â”‚
â”œâ”€â”€ results_analysis.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


---

# ğŸ”§ Installation

## 1ï¸âƒ£ Clone the repository
git clone https://github.com/YOUR_USERNAME/AmbedkarGPT-Intern-Task.git

cd AmbedkarGPT-Intern-Task


## 2ï¸âƒ£ Create & activate virtual environment
python -m venv venv
venv\Scripts\activate # Windows

or

source venv/bin/activate # Mac/Linux

## 3ï¸âƒ£ Install all dependencies
pip install -r requirements.txt


---

# ğŸ¤– Install Ollama + Mistral 7B

### 1. Install Ollama  
Download from: https://ollama.com/download

### 2. Pull Mistral 7B

Make sure the model appears in:

---

# ğŸš€ Running Full Evaluation

To run evaluation for ALL chunk sizes:


This will:

- Build vector DBs (db_small, db_medium, db_large)
- Run RAG on all 25 questions
- Compute all metrics
- Save results inside `/results/`
- Generate combined_report.json

You should see:


---

# ğŸ“Š Output Files

### Located in `/results/`:
- **results_small.json**
- **results_medium.json**
- **results_large.json**
- **combined_report.json**

### You must then write:
- **results_analysis.md**

This file answers:
- Which chunking strategy works best?
- What is the systemâ€™s accuracy?
- Common failure cases?
- What improvements are recommended?

---

# ğŸ¯ Evaluation Metrics Used

### Retrieval Metrics
- Hit@5
- Precision@5
- Mean Reciprocal Rank (MRR)

### Answer Quality
- ROUGE-L
- BLEU
- Faithfulness (context-grounded)
- Relevance

### Semantic Metrics
- Cosine Similarity (sentence-transformers)

---

# ğŸ§ª (Optional) Run Interactive Q&A (CLI)

After DBs are built:

This loads the **medium DB** and lets you ask questions manually.

---

# ğŸ Final Deliverables

You must upload to GitHub:

- [x] evaluation.py  
- [x] utils.py  
- [x] metrics.py  
- [x] test_dataset.json  
- [x] corpus/  
- [x] results/  
- [x] results_analysis.md  
- [x] requirements.txt  
- [x] README.md  

Then submit the GitHub link.

---

# ğŸ‰ Conclusion

This project evaluates the quality of a complete RAG pipeline using:
- Multiple chunk sizes  
- Offline vector search  
- Offline LLM inference  
- NLP metrics  
- Comparative analysis  

It provides a research-style evaluation that helps identify the **best configuration** for RAG performance.

---
